{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FunctionTransformer - Example of Use Within a Pipeline #\n",
    "\n",
    "ATTENTION: the FunctionTransformer of a function has a fit method that ignores the y data: ....fit(X, y) and so does the fit_transform(X, y) <br>\n",
    "\n",
    "That means: the only purpose the FunctionTransformer of a function fulfills is transforming X into an new X.  <br>\n",
    "It is stateless too, i.e. no parameters in a gridsearch are passed into it. That is why I did not use it in the kmeans note-book, though a label-propagator <br>\n",
    "would have been nice to have. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Matt Terry <matt.terry@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"sci.med\", \"sci.space\"]\n",
    "X_train, y_train = fetch_20newsgroups(\n",
    "    random_state=1,\n",
    "    subset=\"train\",\n",
    "    categories=categories,\n",
    "    remove=(\"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "X_test, y_test = fetch_20newsgroups(\n",
    "    random_state=1,\n",
    "    subset=\"test\",\n",
    "    categories=categories,\n",
    "    remove=(\"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_body_extractor(posts):\n",
    "    # construct object dtype array with two columns\n",
    "    # first column = 'subject' and second column = 'body'\n",
    "    features = np.empty(shape=(len(posts), 2), dtype=object)\n",
    "    for i, text in enumerate(posts):\n",
    "        # temporary variable `_` stores '\\n\\n'\n",
    "        headers, _, body = text.partition(\"\\n\\n\")\n",
    "        # store body text in second column\n",
    "        features[i, 1] = body\n",
    "\n",
    "        prefix = \"Subject:\"\n",
    "        sub = \"\"\n",
    "        # save text after 'Subject:' in first column\n",
    "        for line in headers.split(\"\\n\"):\n",
    "            if line.startswith(prefix):\n",
    "                sub = line[len(prefix) :]\n",
    "                break\n",
    "        features[i, 0] = sub\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "subject_body_transformer = FunctionTransformer(subject_body_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stats(posts):\n",
    "    return [{\"length\": len(text), \"num_sentences\": text.count(\".\")} for text in posts]\n",
    "\n",
    "\n",
    "text_stats_transformer = FunctionTransformer(text_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        # Extract subject & body\n",
    "        (\"subjectbody\", subject_body_transformer),\n",
    "        # Use ColumnTransformer to combine the subject and body features\n",
    "        (\n",
    "            \"union\",\n",
    "            ColumnTransformer(\n",
    "                [\n",
    "                    # bag-of-words for subject (col 0)\n",
    "                    (\"subject\", TfidfVectorizer(min_df=50), 0),\n",
    "                    # bag-of-words with decomposition for body (col 1)\n",
    "                    (\n",
    "                        \"body_bow\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\"tfidf\", TfidfVectorizer()),\n",
    "                                (\"best\", TruncatedSVD(n_components=50)),\n",
    "                            ]\n",
    "                        ),\n",
    "                        1,\n",
    "                    ),\n",
    "                    # Pipeline for pulling text stats from post's body\n",
    "                    (\n",
    "                        \"body_stats\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\n",
    "                                    \"stats\",\n",
    "                                    text_stats_transformer,\n",
    "                                ),  # returns a list of dicts\n",
    "                                (\n",
    "                                    \"vect\",\n",
    "                                    DictVectorizer(),\n",
    "                                ),  # list of dicts -> feature matrix\n",
    "                            ]\n",
    "                        ),\n",
    "                        1,\n",
    "                    ),\n",
    "                ],\n",
    "                # weight above ColumnTransformer features\n",
    "                transformer_weights={\n",
    "                    \"subject\": 0.8,\n",
    "                    \"body_bow\": 0.5,\n",
    "                    \"body_stats\": 1.0,\n",
    "                },\n",
    "            ),\n",
    "        ),\n",
    "        # Use a SVC classifier on the combined features\n",
    "        (\"svc\", LinearSVC(dual=False)),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ....... (step 1 of 3) Processing subjectbody, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 3) Processing union, total=   0.3s\n",
      "[Pipeline] ............... (step 3 of 3) Processing svc, total=   0.0s\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       396\n",
      "           1       0.87      0.84      0.85       394\n",
      "\n",
      "    accuracy                           0.86       790\n",
      "   macro avg       0.86      0.86      0.86       790\n",
      "weighted avg       0.86      0.86      0.86       790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Classification report:\\n\\n{}\".format(classification_report(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('condaPytorchEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df61620949cb14e55b843831dad5e2bfed0f7347786fc0c0d43a7e8bd69fd61c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
