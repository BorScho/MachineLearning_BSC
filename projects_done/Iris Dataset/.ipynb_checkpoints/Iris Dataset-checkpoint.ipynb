{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "latin-amber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width        class\n",
       "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "4           5.4          3.9           1.7          0.4  Iris-setosa"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"./iris.data\")\n",
    "iris_df = pd.read_csv(data_path, header=0, names=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"])\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "entitled-planet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sepal length  sepal width  petal length  petal width\n",
      "count    149.000000   149.000000    149.000000   149.000000\n",
      "mean       5.848322     3.051007      3.774497     1.205369\n",
      "std        0.828594     0.433499      1.759651     0.761292\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.400000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n",
      "Class counts: [49 50 50]\n",
      "Number of instances: 149 \n"
     ]
    }
   ],
   "source": [
    "# prelim data overview:\n",
    "\n",
    "print(iris_df.describe())\n",
    "class_no = { \"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\":2 }\n",
    "binc = np.bincount([class_no.get(c) for c in iris_df[\"class\"]])\n",
    "print(f\"Class counts: {binc}\")\n",
    "print(f\"Number of instances: {len(iris_df)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "extended-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "IrisNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "class IrisNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = IrisNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "marine-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define torch.dataset: __init__(), __len__(), __getitem__()\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IrisDataSet(Dataset):\n",
    "    def __init__(self, data_df, transform=None, target_transform=None):\n",
    "        self.iris_df = data_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.X = np.asarray(self.iris_df.iloc[:,:4].values, dtype=np.float32)\n",
    "        self.Y = np.asarray(self.iris_df[\"class\"].values)\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    def __getitem__(self,idx):\n",
    "        self.x = self.X[idx,:]\n",
    "        self.y = self.Y[idx]\n",
    "        if self.transform != None:\n",
    "            self.x = self.transform(self.x)\n",
    "        if self.target_transform != None:\n",
    "            self.y = self.target_transform(self.y)\n",
    "        return self.x, self.y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "proof-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_dataframe(data_df, column_names_to_normalize):\n",
    "    \"\"\"\n",
    "        Normalizes all given columns of a given data frame with a StandardScaler from Sklearn. \n",
    "        Input:\n",
    "            data_df: dataframe with numerical values to normalize\n",
    "            column_names_to_normalize: list of the names of the columns to be normalized\n",
    "        Output:\n",
    "            dataframe with columns normalized\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    data_norm = data_df[column_names_to_normalize].values\n",
    "    data_normed = scaler.fit_transform(data_norm)\n",
    "    df_temp = pd.DataFrame(data_normed, columns=column_names_to_normalize, index=data_df.index)\n",
    "    data_df[column_names_to_normalize]= df_temp\n",
    "    return data_df\n",
    "#print(scaler.mean_)\n",
    "#print(scaler.var_)\n",
    "#print(f\"test normed: {test_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "human-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloading, Normalization:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# read data into data-frame:\n",
    "data_path = Path(\"./iris.data\")\n",
    "iris_df = pd.read_csv(data_path, header=0, names=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"])\n",
    "\n",
    "# split data into train and test:\n",
    "train_df, test_df = train_test_split(iris_df, test_size=0.2)\n",
    "train_df = train_df.copy() # train_df from train_test_split is just a view, i.e. causes problems when normalizing\n",
    "test_df = test_df.copy()\n",
    "\n",
    "# normalize the train and test data:\n",
    "column_names_to_normalize = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "\n",
    "\n",
    "\n",
    "train_df = normalize_dataframe(train_df, column_names_to_nomalize)\n",
    "test_df = normalize_dataframe(test_df, column_names_to_nomalize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "liberal-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class->Number encoding of the labels:\n",
    "\n",
    "class_no = { \"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\":2 }\n",
    "target_transform_class_no = class_no.get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "reverse-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding the labels:\n",
    "from torchvision.transforms import Lambda # might be overkill to call these just for OHE...\n",
    "\n",
    "# simple OHE encoding:\n",
    "#class_ohe = { \"Iris-setosa\": [1,0,0], \"Iris-versicolor\": [0,1,0], \"Iris-virginica\":[0,0,1] }\n",
    "#target_transfrom_class_ohe = class_ohe.get\n",
    "\n",
    "# another OHE encoding supposing, that the labels y have been number-encoded before:\n",
    "transform_ohe = Lambda(lambda y: torch.zeros(3, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "minute-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Transform Definition:\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "target_transform_ohe = Compose([\n",
    "    target_transform_class_no,\n",
    "    #transform_ohe,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets and dataloaders - WITHOUT K-FOLDING:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = IrisDataSet(data_df=train_df, target_transform=target_transform_ohe)\n",
    "test_ds = IrisDataSet(data_df=test_df, target_transform=target_transform_ohe)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "casual-brisbane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "8\n",
      "torch.Size([8])\n",
      "tensor([2, 0, 0, 0, 0, 2, 0, 1])\n",
      "tensor([[0.0000, 0.0000, 0.0012],\n",
      "        [0.6512, 0.2266, 0.0000],\n",
      "        [0.4909, 0.2091, 0.0808],\n",
      "        [0.5595, 0.1986, 0.0302],\n",
      "        [0.5750, 0.2528, 0.0917],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.5019, 0.2088, 0.1056],\n",
      "        [0.2666, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([2, 0, 0, 0, 0, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# test for datasets and loaders\n",
    "pred = torch.Tensor()\n",
    "train_features, train_labels = next(iter(train_dl))\n",
    "print(train_features.shape)\n",
    "print(len(train_labels))\n",
    "print(train_labels.shape)\n",
    "print(train_labels)\n",
    "preds = model(train_features.float())\n",
    "print(preds)\n",
    "print(preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "closing-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and train loops:\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    for n_batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if n_batch % 5 == 0:\n",
    "            print(f\"\\n Batch Loss: {loss.item()} --- Current Sample: {n_batch * len(y)} / {data_size}\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    losses, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (X,y) in dataloader:\n",
    "            pred = model(X)\n",
    "            losses += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1)== y).type(torch.float).sum().item()\n",
    "    \n",
    "    print(f\"\\n Avg. Test Loss per Epoch: {losses/ data_size :.8f}\")\n",
    "    print(f\"\\n Accuracy per Epoch: {correct / data_size: .4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dietary-fleet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Epoch: 0 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 0.9395959377288818 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9424747228622437 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9317812919616699 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12547222\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.0735996961593628 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9870849847793579 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8021038770675659 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12847925\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 0.8989777565002441 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0403140783309937 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9457674026489258 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12904585\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 0.8932464122772217 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.879732608795166 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9861289262771606 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12692974\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 0.9395367503166199 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.0283496379852295 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.0849673748016357 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13347063\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 1 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 0.9044650793075562 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.8768855333328247 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1080093383789062 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12526303\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 0.957128643989563 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0560303926467896 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8744860887527466 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12783563\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.054640769958496 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.8519556522369385 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9218101501464844 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12628497\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 0.9140911102294922 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.935393214225769 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.0518821477890015 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12652074\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 0.9197363257408142 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.0851154327392578 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 0.8544756174087524 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13087253\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 2 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1295759677886963 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0752993822097778 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8635541796684265 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12653467\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.0772887468338013 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.7808738946914673 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8264780640602112 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12730920\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 0.9297084212303162 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0275391340255737 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.0615254640579224 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12783618\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 0.9552094340324402 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9446914196014404 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1093354225158691 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12644655\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 0.9714237451553345 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 0.9484519362449646 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 0.9238772988319397 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13221017\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 3 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 0.9093145132064819 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9351314306259155 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.023635983467102 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12611510\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.037937879562378 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.805038332939148 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1730273962020874 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12650625\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 0.981360912322998 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.83650803565979 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9284614324569702 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12838745\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.0591638088226318 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9063512682914734 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9878531694412231 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12682383\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.0193102359771729 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 0.9946364164352417 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 0.6827731132507324 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13231664\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 4 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 0.9683203101158142 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.026969075202942 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.131360411643982 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12618834\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.0669772624969482 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.078150987625122 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9273006916046143 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12718094\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 0.9324007034301758 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9218740463256836 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.099605917930603 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12703898\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 0.8954735994338989 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9234540462493896 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.0315146446228027 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12549757\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 0.9082683324813843 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 0.8785128593444824 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 0.928548276424408 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13168973\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 5 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 0.8573105335235596 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9631980657577515 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.0674432516098022 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12593260\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.022512435913086 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.8498532176017761 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8627023100852966 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12845636\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.064294695854187 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.8977924585342407 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8863509893417358 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12723004\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.018739104270935 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.032378077507019 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9037488102912903 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12859477\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 0.8147256374359131 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 0.9905198216438293 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.053855538368225 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12940976\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 6 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 0.7809050679206848 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.991806149482727 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8721659779548645 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12573466\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 0.8310319781303406 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.863162100315094 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.042694330215454 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12697815\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch Loss: 1.0093320608139038 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9476437568664551 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9618780612945557 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12754488\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.0645861625671387 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.8907277584075928 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8031513690948486 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12582458\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.108980655670166 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 0.9794446229934692 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 0.8392466306686401 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13315455\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 7 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.0916123390197754 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9331712126731873 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9772619605064392 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12594650\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 0.9024191498756409 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9308512210845947 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.909011721611023 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12744752\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.0685361623764038 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9923388957977295 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.0218144655227661 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12846326\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 0.8818410634994507 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0071310997009277 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.848008930683136 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12764818\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 0.9832034111022949 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 0.9325925707817078 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 0.8591611385345459 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13212127\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 8 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.0182697772979736 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0225218534469604 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9352973103523254 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12565830\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.0088608264923096 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0244394540786743 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9225112795829773 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12660641\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.1061127185821533 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.8759520053863525 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9925616979598999 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12767931\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 0.9772822260856628 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0310198068618774 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9144949316978455 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12742196\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 0.8540146350860596 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 0.9665751457214355 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.110077142715454 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13254172\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "\n",
      "----- Epoch: 9 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 0.9949514865875244 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0427073240280151 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9012643694877625 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12677955\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.0441054105758667 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9103702306747437 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9711659550666809 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12692835\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 0.8147127628326416 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.0031448602676392 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.8577331304550171 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12763061\n",
      "\n",
      " Accuracy per Epoch:  0.5333\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.058718204498291 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 0.9886244535446167 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 0.9649219512939453 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.12590059\n",
      "\n",
      " Accuracy per Epoch:  0.6000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.0362021923065186 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.129567265510559 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 0.8906062841415405 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.13188961\n",
      "\n",
      " Accuracy per Epoch:  0.5517\n",
      "----- Finished Training -----\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "learning_rate = 1e-8\n",
    "epochs = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
    "apply_stratified_kfold = True\n",
    "\n",
    "for ep in range(epochs):\n",
    "    print(f\"\\n----- Epoch: {ep} -----\")\n",
    "    if apply_stratified_kfold:\n",
    "        column_names_to_normalize = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "        X = iris_df[column_names_to_normalize]\n",
    "        y = iris_df[\"class\"].values\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        for fold, (train_ids, test_ids) in enumerate(skf.split(X, y)):\n",
    "            print(f\"Fold: {fold}\")\n",
    "            train_df = iris_df.iloc[train_ids].copy()\n",
    "            train_df = normalize_dataframe(train_df, column_names_to_normalize)\n",
    "            test_df = iris_df.iloc[test_ids].copy()\n",
    "            test_df = normalize_dataframe(test_df, column_names_to_normalize)\n",
    "            train_ds = IrisDataSet(data_df=train_df, target_transform=target_transform_ohe)\n",
    "            test_ds = IrisDataSet(data_df=test_df, target_transform=target_transform_ohe)\n",
    "            train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "            test_dl = DataLoader(test_ds, batch_size=8, shuffle=True)\n",
    "\n",
    "            model.train()\n",
    "            train_loop(train_dl, model, loss_fn, optimizer)\n",
    "            model.eval()\n",
    "            test_loop(test_dl, model, loss_fn)\n",
    "    else:\n",
    "        model.train()\n",
    "        train_loop(train_dl, model, loss_fn, optimizer)\n",
    "        model.eval()\n",
    "        test_loop(test_dl, model, loss_fn)\n",
    "print(\"----- Finished Training -----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-pytorch] *",
   "language": "python",
   "name": "conda-env-anaconda3-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
