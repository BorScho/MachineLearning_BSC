{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width        class\n",
       "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "4           5.4          3.9           1.7          0.4  Iris-setosa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"./iris.data\")\n",
    "iris_df = pd.read_csv(data_path, header=0, names=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"])\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sepal length  sepal width  petal length  petal width\n",
      "count    149.000000   149.000000    149.000000   149.000000\n",
      "mean       5.848322     3.051007      3.774497     1.205369\n",
      "std        0.828594     0.433499      1.759651     0.761292\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.400000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n",
      "Class counts: [49 50 50]\n",
      "Number of instances: 149 \n"
     ]
    }
   ],
   "source": [
    "# prelim data overview:\n",
    "\n",
    "print(iris_df.describe())\n",
    "class_no = { \"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\":2 }\n",
    "binc = np.bincount([class_no.get(c) for c in iris_df[\"class\"]])\n",
    "print(f\"Class counts: {binc}\")\n",
    "print(f\"Number of instances: {len(iris_df)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "IrisNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "class IrisNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = IrisNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define torch.dataset: __init__(), __len__(), __getitem__()\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IrisDataSet(Dataset):\n",
    "    def __init__(self, data_df, transform=None, target_transform=None):\n",
    "        self.iris_df = data_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.X = np.asarray(self.iris_df.iloc[:,:4].values, dtype=np.float32)\n",
    "        self.Y = np.asarray(self.iris_df[\"class\"].values)\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    def __getitem__(self,idx):\n",
    "        self.x = self.X[idx,:]\n",
    "        self.y = self.Y[idx]\n",
    "        if self.transform != None:\n",
    "            self.x = self.transform(self.x)\n",
    "        if self.target_transform != None:\n",
    "            self.y = self.target_transform(self.y)\n",
    "        return self.x, self.y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_dataframe(data_df, column_names_to_normalize):\n",
    "    \"\"\"\n",
    "        Normalizes all given columns of a given data frame with a StandardScaler from Sklearn. \n",
    "        Input:\n",
    "            data_df: dataframe with numerical values to normalize\n",
    "            column_names_to_normalize: list of the names of the columns to be normalized\n",
    "        Output:\n",
    "            dataframe with columns normalized\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    data_norm = data_df[column_names_to_normalize].values\n",
    "    data_normed = scaler.fit_transform(data_norm)\n",
    "    df_temp = pd.DataFrame(data_normed, columns=column_names_to_normalize, index=data_df.index)\n",
    "    data_df[column_names_to_normalize]= df_temp\n",
    "    return data_df\n",
    "#print(scaler.mean_)\n",
    "#print(scaler.var_)\n",
    "#print(f\"test normed: {test_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloading, Normalization:\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\r\n",
    "\r\n",
    "# read data into data-frame:\r\n",
    "data_path = Path(\"./iris.data\")\r\n",
    "iris_df = pd.read_csv(data_path, header=0, names=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"])\r\n",
    "\r\n",
    "# split data into train and test:\r\n",
    "train_df, test_df = train_test_split(iris_df, test_size=0.2)\r\n",
    "train_df = train_df.copy() # train_df from train_test_split is just a view, i.e. causes problems when normalizing\r\n",
    "test_df = test_df.copy()\r\n",
    "\r\n",
    "# normalize the train and test data:\r\n",
    "column_names_to_normalize = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "train_df = normalize_dataframe(train_df, column_names_to_normalize)\r\n",
    "test_df = normalize_dataframe(test_df, column_names_to_normalize)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class->Number encoding of the labels:\n",
    "\n",
    "class_no = { \"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\":2 }\n",
    "target_transform_class_no = class_no.get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding the labels:\n",
    "from torchvision.transforms import Lambda # might be overkill to call these just for OHE...\n",
    "\n",
    "# simple OHE encoding:\n",
    "#class_ohe = { \"Iris-setosa\": [1,0,0], \"Iris-versicolor\": [0,1,0], \"Iris-virginica\":[0,0,1] }\n",
    "#target_transfrom_class_ohe = class_ohe.get\n",
    "\n",
    "# another OHE encoding supposing, that the labels y have been number-encoded before:\n",
    "transform_ohe = Lambda(lambda y: torch.zeros(3, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Transform Definition:\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "target_transform_ohe = Compose([\n",
    "    target_transform_class_no,\n",
    "    #transform_ohe,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets and dataloaders - WITHOUT K-FOLDING:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = IrisDataSet(data_df=train_df, target_transform=target_transform_ohe)\n",
    "test_ds = IrisDataSet(data_df=test_df, target_transform=target_transform_ohe)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "8\n",
      "torch.Size([8])\n",
      "tensor([0, 2, 0, 0, 0, 0, 2, 2])\n",
      "tensor([[0.0000, 0.1726, 0.3914],\n",
      "        [0.0349, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6260, 0.6933],\n",
      "        [0.0000, 0.1345, 0.3755],\n",
      "        [0.0000, 0.0079, 0.1712],\n",
      "        [0.0000, 0.1148, 0.3112],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([2, 0, 2, 2, 2, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# test for datasets and loaders\n",
    "pred = torch.Tensor()\n",
    "train_features, train_labels = next(iter(train_dl))\n",
    "print(train_features.shape)\n",
    "print(len(train_labels))\n",
    "print(train_labels.shape)\n",
    "print(train_labels)\n",
    "preds = model(train_features.float())\n",
    "print(preds)\n",
    "print(preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and train loops:\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    for n_batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if n_batch % 5 == 0:\n",
    "            print(f\"\\n Batch Loss: {loss.item()} --- Current Sample: {n_batch * len(y)} / {data_size}\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    losses, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (X,y) in dataloader:\n",
    "            pred = model(X)\n",
    "            losses += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1)== y).type(torch.float).sum().item()\n",
    "    \n",
    "    print(f\"\\n Avg. Test Loss per Epoch: {losses/ data_size :.8f}\")\n",
    "    print(f\"\\n Accuracy per Epoch: {correct / data_size: .4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Epoch: 0 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1644858121871948 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1123229265213013 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1150720119476318 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15326178\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.1029298305511475 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.136976957321167 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1158716678619385 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15367104\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.1636959314346313 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1201081275939941 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1226904392242432 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15302275\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.1364773511886597 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1371252536773682 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.173823595046997 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15344708\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.145139455795288 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.1520142555236816 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.1764742136001587 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15879044\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 1 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1773712635040283 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.187029242515564 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1384971141815186 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15332823\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.230339527130127 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.129232406616211 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.185861587524414 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15355584\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.1095430850982666 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.115841269493103 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.2034447193145752 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15290795\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.1443489789962769 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1561124324798584 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1773432493209839 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15328842\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.1522367000579834 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.158344030380249 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.1222079992294312 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15820441\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 2 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1776361465454102 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.2011171579360962 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1636953353881836 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15440629\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.1277275085449219 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.2037147283554077 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1656816005706787 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15357624\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.158206820487976 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1971099376678467 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.182922601699829 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15290520\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.1535251140594482 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1753308773040771 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1245410442352295 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15341110\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.1519460678100586 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.2035386562347412 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.2041420936584473 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15891509\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 3 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1340770721435547 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1676452159881592 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1743788719177246 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15394180\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.1646703481674194 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1348251104354858 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1226660013198853 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15391224\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.19160795211792 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.176531434059143 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.154860019683838 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15297769\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.128117561340332 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1271089315414429 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1310148239135742 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15383097\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.2348659038543701 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.1804516315460205 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.1195917129516602 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15924411\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 4 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1343765258789062 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1417086124420166 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1740423440933228 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15370117\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.1106879711151123 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.207720398902893 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1876213550567627 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15368693\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.1063847541809082 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1619130373001099 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1495716571807861 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15362142\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.1435312032699585 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1499547958374023 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1260919570922852 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15365904\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.1820178031921387 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.1545976400375366 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.1436347961425781 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15824891\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 5 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1341369152069092 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.140134572982788 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1183021068572998 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15332267\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.2044228315353394 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1502541303634644 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1698546409606934 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15397072\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.1422688961029053 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1479886770248413 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.140059232711792 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15325588\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.1533061265945435 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1289559602737427 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.115017056465149 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15327578\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.1074774265289307 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.21427321434021 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.1780818700790405 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15937605\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 6 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1616463661193848 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1523411273956299 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1439599990844727 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15385497\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.1092503070831299 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1552149057388306 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1287214756011963 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15346845\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.1604527235031128 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1746385097503662 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1660621166229248 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15281258\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.108140230178833 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1221868991851807 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1691908836364746 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15316896\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.1564775705337524 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.135263442993164 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.1786835193634033 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15907186\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 7 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1418201923370361 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1756759881973267 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1383318901062012 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15340982\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.148937702178955 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1078617572784424 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1781820058822632 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15329008\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.2184507846832275 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1684280633926392 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1985392570495605 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15293937\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.1469697952270508 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.126934289932251 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1674370765686035 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15386955\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.124859094619751 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.1507469415664673 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.1677414178848267 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15922557\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 8 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1353098154067993 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1573874950408936 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1546539068222046 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15399341\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.1100585460662842 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1432249546051025 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.198716163635254 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15329560\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.1381779909133911 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1850759983062744 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1315940618515015 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15275165\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.1401607990264893 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.133702278137207 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1450587511062622 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15326420\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.138171911239624 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.1364132165908813 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.171032428741455 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15922979\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "\n",
      "----- Epoch: 9 -----\n",
      "Fold: 0\n",
      "\n",
      " Batch Loss: 1.1308530569076538 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1504913568496704 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1733555793762207 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15357663\n",
      "\n",
      " Accuracy per Epoch:  0.0333\n",
      "Fold: 1\n",
      "\n",
      " Batch Loss: 1.1743600368499756 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.2173632383346558 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1564607620239258 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15327553\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 2\n",
      "\n",
      " Batch Loss: 1.1400752067565918 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.162731647491455 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.0996167659759521 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15338155\n",
      "\n",
      " Accuracy per Epoch:  0.1000\n",
      "Fold: 3\n",
      "\n",
      " Batch Loss: 1.162461757659912 --- Current Sample: 0 / 119\n",
      "\n",
      " Batch Loss: 1.1285890340805054 --- Current Sample: 40 / 119\n",
      "\n",
      " Batch Loss: 1.1308168172836304 --- Current Sample: 80 / 119\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15377214\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "Fold: 4\n",
      "\n",
      " Batch Loss: 1.146237850189209 --- Current Sample: 0 / 120\n",
      "\n",
      " Batch Loss: 1.1747828722000122 --- Current Sample: 40 / 120\n",
      "\n",
      " Batch Loss: 1.139662742614746 --- Current Sample: 80 / 120\n",
      "\n",
      " Avg. Test Loss per Epoch: 0.15913997\n",
      "\n",
      " Accuracy per Epoch:  0.0000\n",
      "----- Finished Training -----\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "learning_rate = 1e-8\n",
    "epochs = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
    "apply_stratified_kfold = True\n",
    "\n",
    "for ep in range(epochs):\n",
    "    print(f\"\\n----- Epoch: {ep} -----\")\n",
    "    if apply_stratified_kfold:\n",
    "        column_names_to_normalize = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "        X = iris_df[column_names_to_normalize]\n",
    "        y = iris_df[\"class\"].values\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        for fold, (train_ids, test_ids) in enumerate(skf.split(X, y)):\n",
    "            print(f\"Fold: {fold}\")\n",
    "            train_df = iris_df.iloc[train_ids].copy()\n",
    "            train_df = normalize_dataframe(train_df, column_names_to_normalize)\n",
    "            test_df = iris_df.iloc[test_ids].copy()\n",
    "            test_df = normalize_dataframe(test_df, column_names_to_normalize)\n",
    "            train_ds = IrisDataSet(data_df=train_df, target_transform=target_transform_ohe)\n",
    "            test_ds = IrisDataSet(data_df=test_df, target_transform=target_transform_ohe)\n",
    "            train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "            test_dl = DataLoader(test_ds, batch_size=8, shuffle=True)\n",
    "\n",
    "            model.train()\n",
    "            train_loop(train_dl, model, loss_fn, optimizer)\n",
    "            model.eval()\n",
    "            test_loop(test_dl, model, loss_fn)\n",
    "    else:\n",
    "        model.train()\n",
    "        train_loop(train_dl, model, loss_fn, optimizer)\n",
    "        model.eval()\n",
    "        test_loop(test_dl, model, loss_fn)\n",
    "print(\"----- Finished Training -----\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2da2a01206928b59d169f1baf1b7bbbb4ecd1378e37d125bf69781943a3c02b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}