{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Information: ###\n",
    "<br>\n",
    "TASK: Predicting the age of abalone from physical measurements. <br>\n",
    "WHY: The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of <br>\n",
    "rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. \n",
    "<br> \n",
    "WHY DO I WANT TO SOLVE IT:\n",
    "<br> \n",
    "It's a supervised regression problem. I can try: linear regression, polynomial regression.\n",
    "I can make it a classification problem by introducing age-ranges as classes and thus use Decision-Trees, Random Forrests, XGBoost, CatBoost, support vector machines.\n",
    "<br> \n",
    "ASSUMPTIONS:\n",
    "<br> Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.<br>\n",
    "<br>\n",
    "From the original data examples with missing values were removed (the majority having the predicted value missing), and the ranges of the continuous values have been <br>\n",
    "scaled for use with an ANN (by dividing by 200).<br>\n",
    "<br>\n",
    "<br>\n",
    "Attribute Information:<br>\n",
    "<br>\n",
    "Given is the attribute name, attribute type, the measurement unit and a brief description. The number of rings is the value to predict: either as a continuous value or as <br>\n",
    "a classification problem.<br>\n",
    "<br>\n",
    "Name / Data Type / Measurement Unit / Description<br>\n",
    "-----------------------------<br>\n",
    "Sex / nominal / -- / M, F, and I (infant)<br>\n",
    "Length / continuous / mm / Longest shell measurement<br>\n",
    "Diameter / continuous / mm / perpendicular to length<br>\n",
    "Height / continuous / mm / with meat in shell<br>\n",
    "Whole weight / continuous / grams / whole abalone<br>\n",
    "Shucked weight / continuous / grams / weight of meat<br>\n",
    "Viscera weight / continuous / grams / gut weight (after bleeding)<br>\n",
    "Shell weight / continuous / grams / after being dried<br>\n",
    "Rings / integer / -- / +1.5 gives the age in years<br>\n",
    "<br>\n",
    "The readme file contains attribute statistics.<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole</th>\n",
       "      <th>Shucked</th>\n",
       "      <th>Viscera</th>\n",
       "      <th>Shell</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height   Whole  Shucked  Viscera  Shell  Rings\n",
       "0   M   0.455     0.365   0.095  0.5140   0.2245   0.1010  0.150     15\n",
       "1   M   0.350     0.265   0.090  0.2255   0.0995   0.0485  0.070      7\n",
       "2   F   0.530     0.420   0.135  0.6770   0.2565   0.1415  0.210      9\n",
       "3   M   0.440     0.365   0.125  0.5160   0.2155   0.1140  0.155     10\n",
       "4   I   0.330     0.255   0.080  0.2050   0.0895   0.0395  0.055      7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole\", \"Shucked\", \"Viscera\", \"Shell\", \"Rings\"]\n",
    "sourcepath = Path(r\"Data\\abalone.data\".replace(\"\\\\\", \"/\"))\n",
    "data = pd.read_csv(sourcepath, names=columns)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole</th>\n",
       "      <th>Shucked</th>\n",
       "      <th>Viscera</th>\n",
       "      <th>Shell</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height   Whole  Shucked  Viscera  Shell  Sex_F  Sex_I  \\\n",
       "0   0.455     0.365   0.095  0.5140   0.2245   0.1010  0.150      0      0   \n",
       "1   0.350     0.265   0.090  0.2255   0.0995   0.0485  0.070      0      0   \n",
       "2   0.530     0.420   0.135  0.6770   0.2565   0.1415  0.210      1      0   \n",
       "3   0.440     0.365   0.125  0.5160   0.2155   0.1140  0.155      0      0   \n",
       "4   0.330     0.255   0.080  0.2050   0.0895   0.0395  0.055      0      1   \n",
       "\n",
       "   Sex_M  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split off targets:\n",
    "targets = data.iloc[:,-1]\n",
    "data = data.iloc[:,:-1]\n",
    "\n",
    "# one-hot-encode Sex:\n",
    "data = pd.get_dummies(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.to_numpy(), targets.to_numpy(), test_size= 0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3341, 10), (836, 10), (3341,), (836,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-Shooting Algorithms ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = Pipeline([\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"linreg\", LinearRegression())\n",
    "]) \n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(f\"{lin_reg.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_reg = Pipeline([\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"sv_reg\", SVR())\n",
    "    ])\n",
    "\n",
    "svr_reg.fit(X_train, y_train)\n",
    "print(f\"{svr_reg.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "#tree_clf.predict(X_test, y_test)\n",
    "print(f\"{tree_clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train, y_train)\n",
    "print(f\"{tree_reg.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum-Up Spot Shot: ##\n",
    "1. Linear Regression score: 0.548\n",
    "2. Support Vector Regression score: 0.536\n",
    "3. Decision Tree Classification score: 0.214\n",
    "4. Decision Tree Regression score: 0.118\n",
    "<br>\n",
    "<br>\n",
    "So we will go with linear regression and support vector regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Results ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression on KMeans ###\n",
    "\n",
    "Clustering by KMeans reduces variance (and increases bias by los of information) for the lin-reg. If the number of clusters is to smal, the datapoints for the lin-reg become to coarse (high bias), if their number is to high there is increase in bias and to little loss in variance. So probably a grid-search for the number of clusters is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=14.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'kmeans__max_iter': 700, 'kmeans__n_clusters': 100}\n",
      "Best training-score: 0.548\n",
      "Score on test data: 0.558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_lin_reg = Pipeline([\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"kmeans\", KMeans()),\n",
    "    (\"lin_reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    \"kmeans__n_clusters\" : np.arange(10, 200, 10),\n",
    "    \"kmeans__max_iter\" : np.arange(500, 1000, 100),\n",
    "}\n",
    "clf = GridSearchCV(pipe_lin_reg, params, n_jobs=-1, cv=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {clf.best_params_}\")\n",
    "print(f\"Best training-score: {clf.best_score_:.3f}\")\n",
    "print(f\"Score on test data: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression on KMeans and PCA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "420 fits failed out of a total of 4200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "42 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 809, in fit\n",
      "    return self.partial_fit(X, y, sample_weight)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 844, in partial_fit\n",
      "    X = self._validate_data(\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 577, in _validate_data\n",
      "    X = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 918, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 feature(s) (shape=(3006, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "378 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 809, in fit\n",
      "    return self.partial_fit(X, y, sample_weight)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 844, in partial_fit\n",
      "    X = self._validate_data(\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 577, in _validate_data\n",
      "    X = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 918, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 feature(s) (shape=(3007, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.31531765 0.33745192 0.33503599 0.52692551 0.5375723\n",
      " 0.52923238 0.52987838 0.53963683 0.53585882        nan 0.31686251\n",
      " 0.34486096 0.34377774 0.53919466 0.55333767 0.54444015 0.54685344\n",
      " 0.55762038 0.55204093        nan 0.31663873 0.34485874 0.34541113\n",
      " 0.5414727  0.56212412 0.55272793 0.55701224 0.56617873 0.56026614\n",
      "        nan 0.3164365  0.34460966 0.34696181 0.54211915 0.56638766\n",
      " 0.55914934 0.5579667  0.55781139 0.56102524        nan 0.31286371\n",
      " 0.34354053 0.34366438 0.54130665 0.56887129 0.5621924  0.5649642\n",
      " 0.5244181  0.56504966        nan 0.31036045 0.34139561 0.34280448\n",
      " 0.54224493 0.56896512 0.5664451  0.56485739 0.55733813 0.56561397\n",
      "        nan 0.30778718 0.3364261  0.3414383  0.54035569 0.56996859\n",
      " 0.56347682 0.56493233 0.5490937  0.5667794         nan 0.30489079\n",
      " 0.33784658 0.33637484 0.53892733 0.56319506 0.56315313 0.56206909\n",
      " 0.575093   0.56567095        nan 0.30288471 0.33316745 0.33576971\n",
      " 0.53164064 0.57051541 0.56206089 0.56345616 0.57154399 0.53926247\n",
      "        nan 0.29856004 0.33094912 0.33497838 0.53199578 0.5682305\n",
      " 0.56589728 0.56717318 0.5441965  0.44471752        nan 0.30207704\n",
      " 0.32705674 0.32972378 0.53031403 0.56895289 0.56500191 0.5644165\n",
      " 0.58033831 0.45265036        nan 0.29393298 0.32458221 0.32397742\n",
      " 0.52841556 0.56753299 0.56638427 0.56322077 0.4267754  0.51320536\n",
      "        nan 0.29769439 0.31972957 0.3233218  0.52489824 0.56830082\n",
      " 0.56568729 0.56600414 0.56514596 0.38468435        nan 0.29785148\n",
      " 0.31627076 0.32036686 0.5258863  0.56889611 0.56289289 0.56237039\n",
      " 0.55324269 0.54910094        nan 0.31486374 0.33670385 0.33519752\n",
      " 0.52640138 0.53780348 0.53294304 0.52936322 0.54285391 0.53601283\n",
      "        nan 0.31772784 0.34623055 0.34494954 0.54019827 0.55297442\n",
      " 0.54251483 0.54515825 0.54980048 0.55191713        nan 0.31719878\n",
      " 0.34361368 0.3458074  0.54382802 0.56423994 0.55305809 0.55594241\n",
      " 0.56096075 0.558074          nan 0.31359297 0.34327212 0.34453649\n",
      " 0.54317254 0.56502776 0.55472168 0.55817126 0.55346852 0.56608991\n",
      "        nan 0.31193904 0.34189226 0.34385428 0.54294619 0.56687794\n",
      " 0.56120144 0.562992   0.5400016  0.56631645        nan 0.30892549\n",
      " 0.34037768 0.34339705 0.54184592 0.56817288 0.56405028 0.56424257\n",
      " 0.56871433 0.56685009        nan 0.3064061  0.33866193 0.33936814\n",
      " 0.53969743 0.57037676 0.56316301 0.5652659  0.55998949 0.56668953\n",
      "        nan 0.30691673 0.33463265 0.33957922 0.53733988 0.56784741\n",
      " 0.56580194 0.56456473 0.57292949 0.56232667        nan 0.30115869\n",
      " 0.33302762 0.33650899 0.53723136 0.56638766 0.56287827 0.56697787\n",
      " 0.50759914 0.42869102        nan 0.30112    0.33284393 0.33441212\n",
      " 0.5329489  0.56848351 0.56038206 0.56500599 0.57439028 0.56120665\n",
      "        nan 0.29729103 0.32768506 0.32910044 0.53131918 0.56693632\n",
      " 0.56735817 0.56425008 0.57300264 0.5603129         nan 0.29900349\n",
      " 0.32177889 0.32578899 0.52845723 0.56732233 0.5657792  0.56375915\n",
      " 0.52147968 0.54153068        nan 0.29777368 0.32473738 0.32256122\n",
      " 0.53169823 0.56660668 0.56454444 0.56299444 0.57447841 0.56840227\n",
      "        nan 0.29926525 0.31792624 0.32114631 0.52553654 0.56321105\n",
      " 0.56227938 0.56577239 0.42972616 0.21791391        nan 0.31502258\n",
      " 0.33671605 0.33504803 0.52570091 0.53767959 0.53295713 0.52884848\n",
      " 0.54076977 0.53452163        nan 0.31818794 0.34566807 0.34357861\n",
      " 0.53925054 0.55063432 0.54358419 0.54640195 0.55912293 0.55104854\n",
      "        nan 0.31636148 0.3450648  0.34529449 0.54186779 0.56361512\n",
      " 0.55448645 0.55419289 0.56497228 0.55713987        nan 0.31638641\n",
      " 0.34507371 0.34380939 0.54272735 0.5649101  0.55546608 0.55809209\n",
      " 0.56828237 0.56195794        nan 0.31203798 0.34209209 0.34337889\n",
      " 0.54170574 0.56734202 0.55665717 0.56062986 0.56760954 0.56412319\n",
      "        nan 0.30953509 0.33854406 0.34015601 0.54088789 0.5687151\n",
      " 0.56145192 0.56637389 0.56090635 0.54517686        nan 0.30782837\n",
      " 0.3393793  0.33835173 0.53888877 0.56712476 0.56581873 0.56412169\n",
      " 0.57391121 0.56456391        nan 0.30774134 0.33636075 0.33760084\n",
      " 0.53682747 0.56918926 0.56693653 0.56489603 0.55452746 0.49076606\n",
      "        nan 0.30274755 0.33058286 0.33501139 0.534232   0.56591516\n",
      " 0.56638023 0.56724618 0.55674879 0.56397923        nan 0.29719268\n",
      " 0.33114608 0.33346694 0.53081364 0.56828356 0.56466591 0.56268436\n",
      " 0.52142968 0.56787937        nan 0.30119567 0.32494341 0.33134288\n",
      " 0.52673804 0.57038125 0.56822895 0.56308317 0.51316547 0.5693186\n",
      "        nan 0.29440232 0.32567302 0.32491499 0.52801627 0.5678343\n",
      " 0.56296111 0.56192202 0.54871666 0.55002757        nan 0.29426417\n",
      " 0.31715481 0.32171963 0.52495947 0.56670685 0.56554525 0.56323745\n",
      " 0.41219298 0.49705106        nan 0.29378027 0.31550312 0.3202254\n",
      " 0.52637345 0.56616175 0.56647692 0.56784723 0.50270479 0.35241995]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=14.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'kmeans__max_iter': 500, 'kmeans__n_clusters': 110, 'pca__n_components': 8}\n",
      "Best training-score: 0.580\n",
      "Score on test data: 0.578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_lin_reg = Pipeline([\n",
    "    (\"pca\", PCA(n_components=3)),\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"kmeans\", KMeans()),\n",
    "    (\"lin_reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    \"pca__n_components\" : np.arange(10),\n",
    "    \"kmeans__n_clusters\" : np.arange(10, 150, 10),\n",
    "    \"kmeans__max_iter\" : np.arange(500, 800, 100),\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(pipe_lin_reg, params, n_jobs=-1, cv=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {clf.best_params_}\")\n",
    "print(f\"Best training-score: {clf.best_score_:.3f}\")\n",
    "print(f\"Score on test data: {clf.score(X_test, y_test):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two turns with the above classifier result in: <br>\n",
    "<br>\n",
    "1m 33,8s<br>\n",
    "Best parameters: {'kmeans__max_iter': 600, 'kmeans__n_clusters': 60, 'pca__n_components': 8}<br>\n",
    "Best training-score: 0.577<br>\n",
    "Score on test data: 0.581<br>\n",
    "<br>\n",
    "1m 31.7s<br>\n",
    "Best parameters: {'kmeans__max_iter': 500, 'kmeans__n_clusters': 90, 'pca__n_components': 8}<br>\n",
    "Best training-score: 0.575<br>\n",
    "Score on test data: 0.582<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further possible improvements of Lin-Reg: ###\n",
    "\n",
    "1. use PolynomialFeatures instead of/ in addition to KMeans\n",
    "2. use regularization: non-negative least squares, ridge, lasso, elastic-net\n",
    "3. use Bayesian Regression (to chose a different type of regression from scikit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR does not profit from KMeans: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hal9\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=14.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'kmeans__max_iter': 900, 'kmeans__n_clusters': 190}\n",
      "Best training-score: 0.437\n",
      "Score on test data: 0.439\n"
     ]
    }
   ],
   "source": [
    "pipe_svr = Pipeline([\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"kmeans\", KMeans()),\n",
    "    (\"svr\", SVR())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    \"kmeans__n_clusters\" : np.arange(10, 200, 10),\n",
    "    \"kmeans__max_iter\" : np.arange(500, 1000, 100),\n",
    "}\n",
    "clf = GridSearchCV(pipe_svr, params, n_jobs=-1, cv=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {clf.best_params_}\")\n",
    "print(f\"Best training-score: {clf.best_score_:.3f}\")\n",
    "print(f\"Score on test data: {clf.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Result:\n",
    "# Best parameters: {'kmeans__max_iter': 500, 'kmeans__n_clusters': 190}\n",
    "# Best training-score: 0.436\n",
    "# Score on test data: 0.438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression with different Kernels ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: Pipeline(steps=[('pca', PCA(n_components=8)), ('ssc', StandardScaler()),\n",
      "                ('svr', SVR())])\n",
      "Best parameter training: {'pca__n_components': 8, 'svr__kernel': 'rbf'}\n",
      "Score on testset: 0.554\n"
     ]
    }
   ],
   "source": [
    "# if kernel=None then by default kernel=\"rbf\" is used (see: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "# We try polynomial features.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "pipe_svr = Pipeline([\n",
    "    (\"pca\", PCA()),\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"svr\", SVR())\n",
    "\n",
    "])\n",
    "\n",
    "params = {\n",
    "    \"pca__n_components\" : np.arange(1,11),\n",
    "    \"svr__kernel\" : ['sigmoid', 'poly', 'rbf'],\n",
    "    #\"svr__degree\" : np.arange(2,6),\n",
    "}\n",
    "\n",
    "svr_clf = GridSearchCV(pipe_svr, params, n_jobs=-1, cv=10)\n",
    "\n",
    "svr_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best estimator: {svr_clf.best_estimator_}\")\n",
    "print(f\"Best parameter training: {svr_clf.best_params_}\")\n",
    "print(f\"Score on testset: {svr_clf.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Results:\n",
    "# 1.\n",
    "# Best estimator: Pipeline(steps=[('ssc', StandardScaler()), ('svr', SVR(kernel='poly'))])\n",
    "# Best parameter training: {'svr__degree': 3}\n",
    "# Score on testset: 0.495\n",
    "#\n",
    "# 2.\n",
    "# Best estimator: Pipeline(steps=[('pca', PCA(n_components=5)), ('ssc', StandardScaler()),\n",
    "                #('svr', SVR(kernel='poly'))])\n",
    "# Best parameter training: {'pca__n_components': 5, 'svr__degree': 3}\n",
    "# Score on testset: 0.468\n",
    "#\n",
    "# 3.\n",
    "# Best estimator: Pipeline(steps=[('pca', PCA(n_components=10)), ('ssc', StandardScaler()),\n",
    "#                ('svr', SVR(kernel='sigmoid'))])\n",
    "# Best parameter training: {'pca__n_components': 10}\n",
    "# Score on testset: -44.512\n",
    "#\n",
    "# 4.\n",
    "# Best estimator: Pipeline(steps=[('pca', PCA(n_components=8)), ('ssc', StandardScaler()),\n",
    "#                ('svr', SVR())])\n",
    "# Best parameter training: {'pca__n_components': 8}\n",
    "# Score on testset: 0.554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling Lin-Reg and SVR ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Linear-Regression ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score bag-lin-regressor: 0.558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "\n",
    "pipe_lin_reg = Pipeline([\n",
    "    (\"pca\", PCA(n_components=8)),\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"kmeans\", KMeans()),\n",
    "    (\"log_reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "bag_lin_reg = BaggingRegressor(base_estimator=pipe_lin_reg, n_jobs=-1, n_estimators=10)\n",
    "bag_lin_reg.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Test score bag-lin-regressor: {bag_lin_reg.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Result:\n",
    "# Test score bag-lin-regressor: 0.557\n",
    "# similar results for n_estimators = 50, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'kmeans__max_iter': 900, 'kmeans__n_clusters': 190}\n",
      "Best training-score: 0.437\n",
      "Score on test data: 0.439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"pca__n_components\" : np.arange(10),\n",
    "    \"kmeans__n_clusters\" : np.arange(10, 150, 10),\n",
    "    \"kmeans__max_iter\" : np.arange(500, 800, 100),\n",
    "}\n",
    "\n",
    "#clf = GridSearchCV(pipe_lin_reg, params, n_jobs=-1, cv=10)\n",
    "\n",
    "#clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {clf.best_params_}\")\n",
    "print(f\"Best training-score: {clf.best_score_:.3f}\")\n",
    "print(f\"Score on test data: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Support Vector Regression ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score bag-sv-regressor: 0.551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "\n",
    "pipe_svr = Pipeline([\n",
    "    (\"pca\", PCA(n_components=8)),\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"svr\", SVR())\n",
    "])\n",
    "\n",
    "bag_svr = BaggingRegressor(base_estimator=pipe_svr, n_jobs=-1)\n",
    "bag_svr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Test score bag-sv-regressor: {bag_svr.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Result:\n",
    "# 1.\n",
    "# n_estimators = 10\n",
    "# Test score bag-sv-regressor: 0.555\n",
    "# similar results with n_estimators = 30, 50, 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum-Up of BaggingRegressor application: ####\n",
    "Bagging did not improve lin-reg or svr. <br>\n",
    "Suspiciously the scores of bagging lin-reg and svr are almost equal: there might be a mistake in my implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling by Voting: Lin-Reg and SVR ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting-Regressor score: 0.576\n",
      "Voting estimators: [Pipeline(steps=[('pca', PCA(n_components=8)), ('ssc', StandardScaler()),\n",
      "                ('kmeans', KMeans(max_iter=600, n_clusters=60)),\n",
      "                ('log_reg', LinearRegression())]), Pipeline(steps=[('pca', PCA(n_components=8)), ('ssc', StandardScaler()),\n",
      "                ('svr', SVR())])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "pipe_lin_reg = Pipeline([\n",
    "    (\"pca\", PCA(n_components=8)),\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"kmeans\", KMeans(max_iter=600, n_clusters= 60)),\n",
    "    (\"log_reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "pipe_svr = Pipeline([\n",
    "    (\"pca\", PCA(n_components=8)),\n",
    "    (\"ssc\", StandardScaler()),\n",
    "    (\"svr\", SVR())\n",
    "])\n",
    "\n",
    "vr_reg = VotingRegressor([(\"lin_reg\", pipe_lin_reg), (\"svr\", pipe_svr)], n_jobs=-1)\n",
    "vr_reg.fit(X_train, y_train)\n",
    "print(f\"Voting-Regressor score: {vr_reg.score(X_test, y_test):.3f}\")\n",
    "print(f\"Voting estimators: {vr_reg.estimators_}\")\n",
    "\n",
    "# Result:\n",
    "# Voting-Regressor score: 0.578\n",
    "# Voting estimators: [Pipeline(steps=[('pca', PCA(n_components=8)), ('ssc', StandardScaler()),\n",
    "#                ('kmeans', KMeans(max_iter=600, n_clusters=60)),\n",
    "#                ('log_reg', LinearRegression())]), Pipeline(steps=[('pca', PCA(n_components=8)), ('ssc', StandardScaler()),\n",
    "#                ('svr', SVR())])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum-up Voting Regressor ###\n",
    "The voting-regressor consisting of a lin-reg and an svr does not do better than the lin-reg allone.\n",
    "<br>\n",
    "One reason might be, that both are doing well/not-so-well on the same sets of datapoints such that there is no gain in voting. <br> \n",
    "<br>\n",
    "Maybe we could improve by adding one of the weaker models to the ensemble (LogisticRegression, KNeighbourRegression, DecisionTreeRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model / Load Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk:\n",
    "import pickle\n",
    "\n",
    "filename = 'votingRegressor.sav'\n",
    "pickle.dump(vr_reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.576\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk: \n",
    "import pickle\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(f\"{result:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Classifier #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3341, 10), (836, 10), (3341,), (836,), numpy.ndarray)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole\", \"Shucked\", \"Viscera\", \"Shell\", \"Rings\"]\n",
    "sourcepath = Path(r\"Data\\abalone.data\".replace(\"\\\\\", \"/\"))\n",
    "data = pd.read_csv(sourcepath, names=columns)\n",
    "# split off targets:\n",
    "targets = data.iloc[:,-1]\n",
    "data = data.iloc[:,:-1]\n",
    "\n",
    "# one-hot-encode Sex:\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# train-test-split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.to_numpy(), targets.to_numpy(), test_size= 0.2, random_state=random_seed)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch data-set definition:\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class AbaloneDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None, target_transform=None):\n",
    "        self.y = targets\n",
    "        self.X = data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_record = self.X[idx,:]\n",
    "        target = self.y[idx]\n",
    "        if self.transform:\n",
    "            data_record = self.transform(data_record)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return torch.tensor(data_record, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "train = AbaloneDataset(X_train, y_train)\n",
    "test = AbaloneDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition:\n",
    "\n",
    "class NeuralFC(nn.Module):\n",
    "    def __init__(self, input_dim) -> None:\n",
    "        super(NeuralFC, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        age = self.linear_relu_stack(x)\n",
    "        return age\n",
    "\n",
    "\n",
    "in_dim = len(data.iloc[0].to_numpy())\n",
    "model = NeuralFC(in_dim)\n",
    "loss_fct = nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hal9\\anaconda3\\envs\\condaPytorchEnv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Hal9\\anaconda3\\envs\\condaPytorchEnv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - training-loss: 6.251738548278809\n",
      "epoch: 1 - training-loss: 8.10035514831543\n",
      "epoch: 10 - training-loss: 34.10915756225586\n",
      "epoch: 20 - training-loss: 18.47370719909668\n",
      "epoch: 30 - training-loss: 11.678505897521973\n",
      "epoch: 40 - training-loss: 19.546667098999023\n",
      "epoch: 50 - training-loss: 9.081730842590332\n",
      "epoch: 60 - training-loss: 2.097820281982422\n",
      "epoch: 70 - training-loss: 8.043416023254395\n",
      "epoch: 80 - training-loss: 10.138575553894043\n",
      "epoch: 90 - training-loss: 7.662653923034668\n"
     ]
    }
   ],
   "source": [
    "# training the model:\n",
    "\n",
    "epochs = range(100)\n",
    "\n",
    "for epoch in epochs:\n",
    "    loss = 0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(inputs)\n",
    "        loss = loss_fct(y_pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"epoch: {epoch} - training-loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the models prediction accuracy on test-set:\n",
    "\n",
    "def MeanModelPredictionAccuracy(nmodel, test_loader, accepted_proximity= None):\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        if(not accepted_proximity):\n",
    "                accepted_proximity = 0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(test_loader):\n",
    "                y_pred = nmodel(inputs)\n",
    "                y_pred = y_pred.detach().numpy()\n",
    "                y_pred = y_pred.round()\n",
    "                predictions.append(y_pred)\n",
    "                actuals.append(targets.numpy())\n",
    "\n",
    "        predictions = np.concatenate(predictions)\n",
    "        actuals = np.concatenate(actuals)\n",
    "        hits = np.array([abs(p-a) <= accepted_proximity for p,a in zip(predictions, actuals)])\n",
    "        return hits.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6348398683029033"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MeanModelPredictionAccuracy(model, train_loader, accepted_proximity=2.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('condaPytorchEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df61620949cb14e55b843831dad5e2bfed0f7347786fc0c0d43a7e8bd69fd61c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
