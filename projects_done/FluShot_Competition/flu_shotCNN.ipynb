{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Images with an CNN ##\n",
    "<br>\n",
    "Because so many columns are replaced by one-hot-encoding, the records consist nearly only of zero-one entries. <br>\n",
    "In this note-book we reshape each record into a 10 x 10 pseudo-image consisting of zero and ones and train a convolutional network on these data. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: ###\n",
    "It does not work - the accuracy of the CNN is merely above 50 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Models to Consider: ###\n",
    "1. Net-In-Net - one-dimensional CNN\n",
    "2. Logistic Regression with label-smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the csv-files:\n",
    "\n",
    "X_train_df = pd.read_csv(\"training_set_features.csv\")\n",
    "y_train_df = pd.read_csv(\"training_set_labels.csv\")\n",
    "X_test_df = pd.read_csv(\"test_set_features.csv\")\n",
    "\n",
    "#X_train_df.shape\n",
    "# Output:\n",
    "# (26707, 36)\n",
    "\n",
    "#X_test_df.shape\n",
    "# out:\n",
    "# (26708, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot encoding categorical columns:\n",
    "\n",
    "categorial_columns = [\"h1n1_concern\", \"h1n1_knowledge\", \"opinion_h1n1_vacc_effective\", \"opinion_h1n1_risk\", \"opinion_h1n1_sick_from_vacc\",\n",
    "\"opinion_seas_vacc_effective\", \"opinion_seas_risk\", \"opinion_seas_sick_from_vacc\", \"age_group\", \"education\", \"race\", \"sex\", \"income_poverty\", \"marital_status\",\n",
    "\"rent_or_own\", \"employment_status\", \"hhs_geo_region\", \"census_msa\", \"household_adults\", \"household_children\", \"employment_industry\", \"employment_occupation\"]\n",
    "\n",
    "binary_columns = [\"h1n1_concern\", \"h1n1_knowledge\", \"behavioral_antiviral_meds\", \"behavioral_avoidance\", \"behavioral_face_mask\", \"behavioral_wash_hands\", \n",
    "\"behavioral_large_gatherings\", \"behavioral_outside_home\", \"behavioral_touch_face\", \"doctor_recc_h1n1\", \"doctor_recc_seasonal\",\t\"chronic_med_condition\",\n",
    "\"child_under_6_months\",\t\"health_worker\", \"health_insurance\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with 0 for all binary columns:\n",
    "\n",
    "X_train_df[binary_columns] = X_train_df[binary_columns].fillna(0)\n",
    "X_test_df[binary_columns] = X_test_df[binary_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorial one-hot-encoding with drop first and dummy-variable for missing values:\n",
    "\n",
    "X_train_df = pd.concat([X_train_df, pd.get_dummies(X_train_df[categorial_columns], drop_first=True, dummy_na=True)], axis=1)\n",
    "X_train_df.drop(categorial_columns, axis=1, inplace=True)\n",
    "\n",
    "X_test_df = pd.concat([X_test_df, pd.get_dummies(X_test_df[categorial_columns], drop_first=True, dummy_na=True)], axis=1)\n",
    "X_test_df.drop(categorial_columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train eval split:\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "eval_size = 0.1\n",
    "\n",
    "X_train_np, X_eval_np, y_train_np, y_eval_np = train_test_split(X_train_df.iloc[:,1:].to_numpy(), y_train_df.iloc[:,1:].to_numpy(), test_size=eval_size, shuffle=True)\n",
    "\n",
    "# output types are numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_h1n1: (21365,)\n",
      "Shape of y_train_seasonal: (21365,)\n",
      "Shape of y_test_h1n1: (5342,)\n",
      "Shape of y_test_seasonal: (5342,)\n"
     ]
    }
   ],
   "source": [
    "# prepare the labels for the two predictions we have to make:\n",
    "\n",
    "#y_train_h1n1 = y_train_np[:,:1].ravel()\n",
    "#y_train_seasonal = y_train_np[:,1:].ravel()\n",
    "#y_eval_h1n1 = y_eval_np[:,:1].ravel()\n",
    "#y_eval_seasonal = y_eval_np[:,1:].ravel()\n",
    "\n",
    "#for s, a in zip([\"y_train_h1n1\", \"y_train_seasonal\", \"y_eval_h1n1\", \"y_eval_seasonal\"], [y_train_h1n1, y_train_seasonal, y_eval_h1n1, y_eval_seasonal]):\n",
    "#    print(f\"Shape of {s}: {a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'only seasonal'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the two-valued y's to 4 categories:\n",
    "\n",
    "# y = 0 ,0 -> 0 - \"not vaccinated\"\n",
    "# 1, 0 -> 1 = \"only seasonal\"\n",
    "# 0, 1 -> 2 = \"only h1n1\"\n",
    "# 1, 1 -> 3 = \"seasonal and h1n1\"\n",
    "\n",
    "flu_shot_categories = { 0: \"not vaccinated\", 1: \"only seasonal\", 2 : \"only h1n1\", 3 : \"seasonal and h1n1\"}\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def multiLableTocategory(l):\n",
    "    if np.array_equal(l, [0,0]):\n",
    "        return 0\n",
    "    if np.array_equal(l,[0,1]):\n",
    "        return 1\n",
    "    if np.array_equal(l, [1,0]):\n",
    "        return 2\n",
    "    if np.array_equal(l, [1,1]):\n",
    "        return 3\n",
    "\n",
    "# test\n",
    "multiLableTocategory([0,1])\n",
    "#out:\n",
    "# 1\n",
    "flu_shot_categories[multiLableTocategory([0,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN ###\n",
    "Pad the (1, 95) feature vectors to (1, 100) and reshape to a 10 x 10 image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class fluNet(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(fluNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size=3,padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.flattened = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(200, 16)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.act1(self.conv1(x)))\n",
    "        x = self.act2(self.conv2(x))\n",
    "        x = self.flattened(x)\n",
    "        x = self.act3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        p = F.log_softmax(x, dim=0)\n",
    "        return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " T.shape: torch.Size([2, 1, 10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6972, -0.7027, -0.7024, -0.6982],\n",
       "        [-0.6891, -0.6837, -0.6840, -0.6881]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if fluNet works nummerically:\n",
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "T = torch.rand(batch_size, 1, 10, 10)\n",
    "print(f\" T.shape: {T.shape}\")\n",
    "\n",
    "flu_clf = fluNet(batch_size)\n",
    "#print(flu_clf)\n",
    "flu_clf(torch.Tensor(T))\n",
    "\n",
    "# out:\n",
    "# T.shape: torch.Size([1, 1, 10, 10])\n",
    "# tensor([-1.2340, -1.3331, -1.4486, -1.5591], grad_fn=<LogSoftmaxBackward>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        for x, y in train_loader:\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "            if epoch ==1 or epoch % 10 ==0:\n",
    "                print(f\"Epoch: {epoch}, Training loss: {loss_train / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model, loss_fn, train_loader, test_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"test\", test_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for xs,ys in loader:\n",
    "                outs = model(xs)\n",
    "                y_preds = torch.argmax(outs, dim=1)\n",
    "                total += y_preds.shape[0]\n",
    "                correct += (y_preds == ys).sum().item()\n",
    "        \n",
    "        print(f\"Accuracy {name}: {correct/ total:.2} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_eval_loop(n_epochs, optimizer, model, loss_fn, train_loader, eval_loader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0\n",
    "        loss_eval = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for x, y in train_loader:\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for ex, ey in eval_loader:\n",
    "                eout = model(ex)\n",
    "                eloss = loss_fn(eout, ey)\n",
    "                loss_eval += eloss.item()\n",
    "                y_preds = torch.argmax(eout, dim=1)\n",
    "                total += y_preds.shape[0]\n",
    "                correct += (y_preds == ey).sum().item()\n",
    "\n",
    "\n",
    "        if epoch == 1 or epoch % 20 == 0:\n",
    "            print(f\"Epoch: {epoch}, Training loss: {loss_train / len(train_loader):.3}\")\n",
    "            print(f\"Epoch evaluation loss: {loss_eval / len(eval_loader):.3}\")\n",
    "            print(f\"Epoch evaluation accuracy: {(correct/ total):.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# data_np is a numpy array with shape (Batch x 1 x W x H) = (NumberOfImages_in_batch, 1, width, hight); batching will be done by the dataloader class\n",
    "\n",
    "class pseudoImagesDataset(Dataset):\n",
    "    def __init__(self, data_np, labels_np):\n",
    "        self.pseudoImages = torch.tensor(data_np.astype(\"float32\"))\n",
    "        self.labels = torch.tensor(labels_np, dtype= torch.long)\n",
    "    def __len__(self):\n",
    "        return self.pseudoImages.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.pseudoImages[idx,:,:]\n",
    "        label = self.labels[idx]\n",
    "        return (img, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'>\n",
      "24036\n"
     ]
    }
   ],
   "source": [
    "# convert all entries of y_train_np (and y_test_np) and stack them:\n",
    "\n",
    "y_train_c = np.array([multiLableTocategory(r) for r in y_train_np]).astype(\"int\")\n",
    "y_eval_c = np.array([multiLableTocategory(r) for r in y_eval_np]).astype(\"int\")\n",
    "\n",
    "# check:\n",
    "print(type(y_train_c[0]))\n",
    "print(len(y_train_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_imgs.shape: (24036, 1, 10, 10)\n",
      "X_eval_imgs.shape: (2671, 1, 10, 10)\n",
      "X_test_imgs.shape: (26708, 1, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pre_pad = np.zeros((X_train_np.shape[0], 5))\n",
    "X_train_pad = np.concatenate((pre_pad, X_train_np), axis=1)\n",
    "X_train_imgs = np.reshape(X_train_pad, (X_train_pad.shape[0], 1, 10,10))\n",
    "\n",
    "print(f\"X_train_imgs.shape: {X_train_imgs.shape}\")\n",
    "## out:\n",
    "# (21365, 10, 10)\n",
    "## check:\n",
    "#img0=X_train_imgs[0,:,:]\n",
    "#print(img0)\n",
    "\n",
    "pre_pad = np.zeros((X_eval_np.shape[0], 5))\n",
    "X_eval_pad = np.concatenate((pre_pad, X_eval_np), axis=1)\n",
    "X_eval_imgs = np.reshape(X_eval_pad, (X_eval_pad.shape[0], 1, 10, 10))\n",
    "\n",
    "print(f\"X_eval_imgs.shape: {X_eval_imgs.shape}\")\n",
    "\n",
    "X_test_np = X_test_df.iloc[:,1:].to_numpy()\n",
    "pre_pad = np.zeros((X_test_np.shape[0], 5))\n",
    "X_test_pad = np.concatenate((pre_pad, X_test_np), axis=1)\n",
    "X_test_imgs = np.reshape(X_test_pad, (X_test_pad.shape[0], 1, 10, 10))\n",
    "\n",
    "print(f\"X_test_imgs.shape: {X_test_imgs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 4.0\n",
      "Epoch evaluation loss: 3.95\n",
      "Epoch evaluation accuracy: 0.51\n",
      "Epoch: 20, Training loss: 3.92\n",
      "Epoch evaluation loss: 3.96\n",
      "Epoch evaluation accuracy: 0.54\n",
      "Epoch: 40, Training loss: 3.9\n",
      "Epoch evaluation loss: 3.96\n",
      "Epoch evaluation accuracy: 0.53\n",
      "Epoch: 60, Training loss: 3.89\n",
      "Epoch evaluation loss: 3.97\n",
      "Epoch evaluation accuracy: 0.51\n",
      "Epoch: 80, Training loss: 3.89\n",
      "Epoch evaluation loss: 3.98\n",
      "Epoch evaluation accuracy: 0.52\n",
      "Epoch: 100, Training loss: 3.88\n",
      "Epoch evaluation loss: 3.99\n",
      "Epoch evaluation accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_set = pseudoImagesDataset(X_train_imgs, y_train_c)\n",
    "eval_set = pseudoImagesDataset(X_eval_imgs, y_eval_c)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# train the model:\n",
    "\n",
    "n_epochs = 100\n",
    "model = fluNet(batch_size)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "#loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "\n",
    "training_eval_loop(n_epochs = n_epochs, optimizer = optimizer, model = model, loss_fn = loss_fn, train_loader = train_loader, eval_loader = eval_loader)\n",
    "\n",
    "#y_test = model(X_test_imgs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('condaPytorchEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df61620949cb14e55b843831dad5e2bfed0f7347786fc0c0d43a7e8bd69fd61c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
